{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:55:02.356355309Z",
     "start_time": "2024-03-28T12:55:02.282062064Z"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "import tempfile\n",
    "import uuid\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchrl.collectors import MultiaSyncDataCollector\n",
    "from torchrl.data import LazyMemmapStorage, MultiStep,TensorDictReplayBuffer\n",
    "from torchrl.envs import (\n",
    "    EnvCreator,\n",
    "    ExplorationType,\n",
    "    ParallelEnv,\n",
    "    RewardScaling,\n",
    "    StepCounter,\n",
    ")\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.envs.transforms import (\n",
    "    CatFrames,\n",
    "    Compose,\n",
    "    GrayScale,\n",
    "    ObservationNorm,\n",
    "    Resize,\n",
    "    ToTensorImage,\n",
    "    TransformedEnv,\n",
    ")\n",
    "from torchrl.modules import DuelingCnnDQNet, EGreedyModule, QValueActor\n",
    "from tensordict.nn import TensorDictSequential\n",
    "\n",
    "\n",
    "from torchrl.objectives import DQNLoss, SoftUpdate\n",
    "from torchrl.record.loggers.csv import CSVLogger\n",
    "from torchrl.trainers import (\n",
    "    LogReward,\n",
    "    Recorder,\n",
    "    ReplayBufferTrainer,\n",
    "    Trainer,\n",
    "    UpdateWeights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = torch.device(\"cpu\")\n",
    "# the learning rate of the optimizer\n",
    "lr = 2e-3\n",
    "# weight decay\n",
    "wd = 1e-5\n",
    "# the beta parameters of Adam\n",
    "betas = (0.9, 0.999)\n",
    "# Optimization steps per batch collected (aka UPD or updates per data)\n",
    "n_optim = 8\n",
    "\n",
    "\n",
    "# gamma decay factor\n",
    "gamma = 0.99\n",
    "\n",
    "# Smooth target network update decay parameter.\n",
    "# This loosely corresponds to a 1/tau interval with hard target network update\n",
    "tau = 0.02\n",
    "\n",
    "###############################################################################\n",
    "# Data collection and replay buffer\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# .. note::\n",
    "#   Values to be used for proper training have been commented.\n",
    "#\n",
    "# Total frames collected in the environment. In other implementations, the\n",
    "# user defines a maximum number of episodes.\n",
    "# This is harder to do with our data collectors since they return batches\n",
    "# of N collected frames, where N is a constant.\n",
    "# However, one can easily get the same restriction on the number of episodes by\n",
    "# breaking the training loop when a certain number of episodes has been collected.\n",
    "total_frames = 5_000  # 500000\n",
    "\n",
    "# Random frames used to initialize the replay buffer.\n",
    "init_random_frames = 100  # 1000\n",
    "\n",
    "# Frames in each batch collected.\n",
    "frames_per_batch = 32  # 128\n",
    "\n",
    "# Frames sampled from the replay buffer at each optimization step\n",
    "batch_size = 32  # 256\n",
    "\n",
    "# Size of the replay buffer in terms of frames\n",
    "buffer_size = min(total_frames, 100000)\n",
    "\n",
    "# Number of environments run in parallel in each data collector\n",
    "num_workers = 2  # 8\n",
    "num_collectors = 2  # 4\n",
    "\n",
    "###############################################################################\n",
    "# Environment and exploration\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# We set the initial and final value of the epsilon factor in Epsilon-greedy exploration.\n",
    "# Since our policy is deterministic, exploration is crucial: without it, the\n",
    "# only source of randomness would be the environment reset.\n",
    "\n",
    "eps_greedy_val = 0.1\n",
    "eps_greedy_val_env = 0.005\n",
    "\n",
    "# To speed up learning, we set the bias of the last layer of our value network\n",
    "# to a predefined value (this is not mandatory)\n",
    "init_bias = 2.0\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:55:02.361692117Z",
     "start_time": "2024-03-28T12:55:02.328139975Z"
    }
   },
   "id": "10d94f5a1c63619d"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def make_env(\n",
    "    parallel=False,\n",
    "    obs_norm_sd=None,\n",
    "):\n",
    "    if obs_norm_sd is None:\n",
    "        obs_norm_sd = {\"standard_normal\": True}\n",
    "    if parallel:\n",
    "        base_env = ParallelEnv(\n",
    "            num_workers,\n",
    "            EnvCreator(\n",
    "                lambda: GymEnv(\n",
    "                    \"CartPole-v1\",\n",
    "                    from_pixels=True,\n",
    "                    pixels_only=True,\n",
    "                    device=device,\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        base_env = GymEnv(\n",
    "            \"CartPole-v1\",\n",
    "            from_pixels=True,\n",
    "            pixels_only=True,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    env = TransformedEnv(\n",
    "        base_env,\n",
    "        Compose(\n",
    "            StepCounter(),  # to count the steps of each trajectory\n",
    "            ToTensorImage(),\n",
    "            RewardScaling(loc=0.0, scale=0.1),\n",
    "            GrayScale(),\n",
    "            Resize(64, 64),\n",
    "            CatFrames(4, in_keys=[\"pixels\"], dim=-3),\n",
    "            ObservationNorm(in_keys=[\"pixels\"], **obs_norm_sd),\n",
    "        ),\n",
    "    )\n",
    "    return env"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:55:02.361900597Z",
     "start_time": "2024-03-28T12:55:02.328504273Z"
    }
   },
   "id": "d11e722ef9d3f740"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "def get_norm_stats():\n",
    "    test_env = make_env()\n",
    "    test_env.transform[-1].init_stats(\n",
    "        num_iter=1000, cat_dim=0, reduce_dim=[-1, -2, -4], keep_dims=(-1, -2)\n",
    "    )\n",
    "    obs_norm_sd = test_env.transform[-1].state_dict()\n",
    "    # let's check that normalizing constants have a size of ``[C, 1, 1]`` where\n",
    "    # ``C=4`` (because of :class:`~torchrl.envs.CatFrames`).\n",
    "    print(\"state dict of the observation norm:\", obs_norm_sd)\n",
    "    test_env.close()\n",
    "    return obs_norm_sd\n",
    "# get_norm_stats()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:55:02.362206158Z",
     "start_time": "2024-03-28T12:55:02.331177287Z"
    }
   },
   "id": "25e28ef9ada8ea6e"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "def make_model(dummy_env):\n",
    "    cnn_kwargs = {\n",
    "        \"num_cells\": [32, 64, 64],\n",
    "        \"kernel_sizes\": [6, 4, 3],\n",
    "        \"strides\": [2, 2, 1],\n",
    "        \"activation_class\": nn.ELU\n",
    "    }\n",
    "    mlp_kwargs = {\n",
    "        \"depth\": 2,\n",
    "        \"num_cells\": [64, 64],\n",
    "        \"activation_class\": nn.ELU,\n",
    "    }\n",
    "    net = DuelingCnnDQNet(dummy_env.action_spec.shape[-1], 1, cnn_kwargs, mlp_kwargs).to(device)\n",
    "    net.value[-1].bias.data.fill_(init_bias)\n",
    "    actor = QValueActor(net, in_keys=[\"pixels\"], spec=dummy_env.action_spec).to(device)\n",
    "    # init LazyNets\n",
    "    tensordict = dummy_env.fake_tensordict()\n",
    "    actor(tensordict)\n",
    "    \n",
    "    # we joint our actor with an EGreedyModule for data collection\n",
    "    exploration_module = EGreedyModule(\n",
    "        spec=dummy_env.action_spec,\n",
    "        annealing_num_steps=total_frames,\n",
    "        eps_init=eps_greedy_val,\n",
    "        eps_end=eps_greedy_val_env\n",
    "    )\n",
    "    actor_explore = TensorDictSequential(actor, exploration_module)\n",
    "    return actor, actor_explore"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:55:02.397385840Z",
     "start_time": "2024-03-28T12:55:02.349515281Z"
    }
   },
   "id": "63db3557f1062420"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "def get_replay_buffer(buffer_size, n_optim, batch_size):\n",
    "    replay_buffer = TensorDictReplayBuffer(\n",
    "        batch_size=batch_size,\n",
    "        storage=LazyMemmapStorage(buffer_size),\n",
    "        prefetch=n_optim,\n",
    "    )\n",
    "    return replay_buffer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:55:02.397646353Z",
     "start_time": "2024-03-28T12:55:02.396371238Z"
    }
   },
   "id": "3d3b67e30f707f17"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "def get_collector(\n",
    "    stats,\n",
    "    num_collectors,\n",
    "    actor_explore,\n",
    "    frames_per_batch,\n",
    "    total_frames,\n",
    "    device,\n",
    "):\n",
    "    cls = MultiaSyncDataCollector\n",
    "    env_arg = [make_env(parallel=True, obs_norm_sd=stats)] * num_collectors\n",
    "    data_collector = cls(\n",
    "        env_arg,\n",
    "        policy=actor_explore,\n",
    "        frames_per_batch=frames_per_batch,\n",
    "        total_frames=total_frames,\n",
    "        # this is the default behaviour: the collector runs in ``\"random\"`` (or explorative) mode\n",
    "        exploration_type=ExplorationType.RANDOM,\n",
    "        # We set the all the devices to be identical. Below is an example of\n",
    "        # heterogeneous devices\n",
    "        device=device,\n",
    "        storing_device=device,\n",
    "        split_trajs=False,\n",
    "        postproc=MultiStep(gamma=gamma, n_steps=5),\n",
    "    )\n",
    "    return data_collector"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:55:02.397754330Z",
     "start_time": "2024-03-28T12:55:02.396592779Z"
    }
   },
   "id": "2323be97e3380a5c"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "def get_loss_module(actor, gamma):\n",
    "    loss_module = DQNLoss(actor, delay_value=True)\n",
    "    loss_module.make_value_estimator(gamma=gamma)\n",
    "    target_updater = SoftUpdate(loss_module, eps=0.995)\n",
    "    return loss_module, target_updater"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:55:02.397843030Z",
     "start_time": "2024-03-28T12:55:02.396736445Z"
    }
   },
   "id": "72c0b529826b20cd"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kostiskak/anaconda3/envs/rl_env/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001B[33mWARN: env.num_envs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_envs` for environment variables or `env.get_wrapper_attr('num_envs')` that will search the reminding wrappers.\u001B[0m\n",
      "  logger.warn(\n",
      "/home/kostiskak/anaconda3/envs/rl_env/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001B[33mWARN: env.reward_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward_space` for environment variables or `env.get_wrapper_attr('reward_space')` that will search the reminding wrappers.\u001B[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state dict of the observation norm: OrderedDict([('standard_normal', tensor(True)), ('loc', tensor([[[0.9895]],\n",
      "\n",
      "        [[0.9895]],\n",
      "\n",
      "        [[0.9895]],\n",
      "\n",
      "        [[0.9895]]])), ('scale', tensor([[[0.0737]],\n",
      "\n",
      "        [[0.0737]],\n",
      "\n",
      "        [[0.0737]],\n",
      "\n",
      "        [[0.0737]]]))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kostiskak/anaconda3/envs/rl_env/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001B[33mWARN: env.num_envs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_envs` for environment variables or `env.get_wrapper_attr('num_envs')` that will search the reminding wrappers.\u001B[0m\n",
      "  logger.warn(\n",
      "/home/kostiskak/anaconda3/envs/rl_env/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001B[33mWARN: env.reward_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward_space` for environment variables or `env.get_wrapper_attr('reward_space')` that will search the reminding wrappers.\u001B[0m\n",
      "  logger.warn(\n",
      "/home/kostiskak/anaconda3/envs/rl_env/lib/python3.11/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "/home/kostiskak/anaconda3/envs/rl_env/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001B[33mWARN: env.num_envs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_envs` for environment variables or `env.get_wrapper_attr('num_envs')` that will search the reminding wrappers.\u001B[0m\n",
      "  logger.warn(\n",
      "/home/kostiskak/anaconda3/envs/rl_env/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001B[33mWARN: env.reward_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward_space` for environment variables or `env.get_wrapper_attr('reward_space')` that will search the reminding wrappers.\u001B[0m\n",
      "  logger.warn(\n",
      "/home/kostiskak/ntua/Robotalk/rl/torchrl/collectors/collectors.py:1377: UserWarning: total_frames (5000) is not exactly divisible by frames_per_batch (32).This means 24 additional frames will be collected.To silence this message, set the environment variable RL_WARNINGS to False.\n",
      "  warnings.warn(\n",
      "/home/kostiskak/ntua/Robotalk/tensordict/tensordict/_pytree.py:147: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  register_pytree_node(\n",
      "/home/kostiskak/ntua/Robotalk/tensordict/tensordict/_pytree.py:147: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  register_pytree_node(\n",
      "/home/kostiskak/ntua/Robotalk/tensordict/tensordict/_pytree.py:147: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  register_pytree_node(\n",
      "/home/kostiskak/ntua/Robotalk/tensordict/tensordict/_pytree.py:147: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  register_pytree_node(\n",
      "/home/kostiskak/ntua/Robotalk/tensordict/tensordict/_pytree.py:147: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  register_pytree_node(\n",
      "/home/kostiskak/ntua/Robotalk/tensordict/tensordict/_pytree.py:147: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  register_pytree_node(\n",
      "/tmp/ipykernel_24519/3237565706.py:22: UserWarning: log dir: /tmp/tmpf8lzg0h2/dqn_exp_693b2e34-ed02-11ee-8c8d-68545a6c9297\n",
      "  warnings.warn(f\"log dir: {logger.experiment.log_dir}\")\n"
     ]
    }
   ],
   "source": [
    "stats = get_norm_stats()\n",
    "# how come here the parallel is False and above the parallel be true?\n",
    "test_env = make_env(parallel=False, obs_norm_sd=stats)\n",
    "# Get model\n",
    "actor, actor_explore = make_model(test_env)\n",
    "loss_module, target_net_updater = get_loss_module(actor, gamma)\n",
    "\n",
    "collector = get_collector(\n",
    "    stats=stats,\n",
    "    num_collectors=num_collectors,\n",
    "    actor_explore=actor_explore,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    "    device=device,\n",
    ")\n",
    "optimizer = torch.optim.Adam(\n",
    "    loss_module.parameters(), lr=lr, weight_decay=wd, betas=betas\n",
    ")\n",
    "exp_name = f\"dqn_exp_{uuid.uuid1()}\"\n",
    "tmpdir = tempfile.TemporaryDirectory()\n",
    "logger = CSVLogger(exp_name=exp_name, log_dir=tmpdir.name)\n",
    "warnings.warn(f\"log dir: {logger.experiment.log_dir}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:55:10.630728914Z",
     "start_time": "2024-03-28T12:55:02.396850428Z"
    }
   },
   "id": "608ec22ed7f4e2de"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "log_interval = 500\n",
    "\n",
    "trainer = Trainer(\n",
    "    collector=collector,\n",
    "    total_frames=total_frames,\n",
    "    frame_skip=1,\n",
    "    loss_module=loss_module,\n",
    "    optimizer=optimizer,\n",
    "    logger=logger,\n",
    "    optim_steps_per_batch=n_optim,\n",
    "    log_interval=log_interval\n",
    ")\n",
    "\n",
    "buffer_hook = ReplayBufferTrainer(\n",
    "    get_replay_buffer(buffer_size, n_optim, batch_size=batch_size),\n",
    "    flatten_tensordicts=True,\n",
    ")\n",
    "buffer_hook.register(trainer)\n",
    "weight_updater = UpdateWeights(collector, update_weights_interval=1)\n",
    "weight_updater.register(trainer)\n",
    "recorder = Recorder(\n",
    "    record_interval=100,  # log every 100 optimization steps\n",
    "    record_frames=1000,  # maximum number of frames in the record\n",
    "    frame_skip=1,\n",
    "    policy_exploration=actor_explore,\n",
    "    environment=test_env,\n",
    "    exploration_type=ExplorationType.MODE,\n",
    "    log_keys=[(\"next\", \"reward\")],\n",
    "    out_keys={(\"next\", \"reward\"): \"rewards\"},\n",
    "    log_pbar=True,\n",
    ")\n",
    "recorder.register(trainer)\n",
    "\n",
    "trainer.register_op(\"post_steps\", actor_explore[1].step, frames=frames_per_batch)\n",
    "trainer.register_op(\"post_optim\", target_net_updater.step)\n",
    "\n",
    "log_reward = LogReward(log_pbar=True)\n",
    "log_reward.register(trainer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T13:05:23.708305883Z",
     "start_time": "2024-03-28T13:05:23.655992089Z"
    }
   },
   "id": "fdcdc143d746f4cc"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]/home/kostiskak/ntua/Robotalk/rl/torchrl/envs/utils.py:160: UserWarning: The expected key set and actual key set differ. This will work but with a slower throughput than when the specs match exactly the actual key set in the data. Expected - Actual keys=set(), \n",
      "Actual - Expected keys={'chosen_action_value', 'action_value'}.\n",
      "  warnings.warn(\n",
      "r_training: 0.4948, rewards: 0.1000, total_rewards: 5.2632: : 5024it [01:27, 68.60it/s]                        "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T13:06:56.444049766Z",
     "start_time": "2024-03-28T13:05:27.261807315Z"
    }
   },
   "id": "44eeb6232746f778"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
