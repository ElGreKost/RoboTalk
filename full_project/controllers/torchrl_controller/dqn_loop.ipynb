{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fee2a92fa43b90f2",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-27T09:15:53.265329325Z",
     "start_time": "2024-03-27T09:15:53.220171966Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kostiskak/anaconda3/envs/rl_env/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001B[33mWARN: env.num_envs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_envs` for environment variables or `env.get_wrapper_attr('num_envs')` that will search the reminding wrappers.\u001B[0m\n",
      "  logger.warn(\n",
      "/home/kostiskak/anaconda3/envs/rl_env/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001B[33mWARN: env.reward_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward_space` for environment variables or `env.get_wrapper_attr('reward_space')` that will search the reminding wrappers.\u001B[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "OneHotDiscreteTensorSpec(\n    shape=torch.Size([3]),\n    space=DiscreteBox(n=3),\n    device=cpu,\n    dtype=torch.int64,\n    domain=discrete)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import time\n",
    "\n",
    "from torchrl.envs import GymEnv, StepCounter, TransformedEnv\n",
    "\n",
    "env = TransformedEnv(GymEnv(\"Acrobot-v1\"), StepCounter())\n",
    "env.set_seed(0)\n",
    "env.action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kostiskak/ntua/Robotalk/tensordict/tensordict/_pytree.py:147: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  register_pytree_node(\n",
      "/home/kostiskak/anaconda3/envs/rl_env/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001B[33mWARN: env.num_envs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_envs` for environment variables or `env.get_wrapper_attr('num_envs')` that will search the reminding wrappers.\u001B[0m\n",
      "  logger.warn(\n",
      "/home/kostiskak/anaconda3/envs/rl_env/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001B[33mWARN: env.reward_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward_space` for environment variables or `env.get_wrapper_attr('reward_space')` that will search the reminding wrappers.\u001B[0m\n",
      "  logger.warn(\n",
      "/home/kostiskak/anaconda3/envs/rl_env/lib/python3.11/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "/home/kostiskak/anaconda3/envs/rl_env/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001B[33mWARN: env.num_envs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_envs` for environment variables or `env.get_wrapper_attr('num_envs')` that will search the reminding wrappers.\u001B[0m\n",
      "  logger.warn(\n",
      "/home/kostiskak/anaconda3/envs/rl_env/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001B[33mWARN: env.reward_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward_space` for environment variables or `env.get_wrapper_attr('reward_space')` that will search the reminding wrappers.\u001B[0m\n",
      "  logger.warn(\n",
      "/home/kostiskak/ntua/Robotalk/rl/torchrl/envs/utils.py:160: UserWarning: The expected key set and actual key set differ. This will work but with a slower throughput than when the specs match exactly the actual key set in the data. Expected - Actual keys=set(), \n",
      "Actual - Expected keys={('collector', 'traj_ids')}.\n",
      "  warnings.warn(\n",
      "2024-03-27 11:15:36,421 [torchrl][INFO] solved after 0 steps, 0 episodes and in 0.12496137619018555s.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x4 and 6x64)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;31mRuntimeError\u001B[0m: TensorDictModule failed with operation\n    MLP(\n      (0): Linear(in_features=6, out_features=64, bias=True)\n      (1): Tanh()\n      (2): Linear(in_features=64, out_features=64, bias=True)\n      (3): Tanh()\n      (4): Linear(in_features=64, out_features=3, bias=True)\n    )\n    in_keys=['observation']\n    out_keys=['action_value'].",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 94\u001B[0m\n\u001B[1;32m     89\u001B[0m torchrl_logger\u001B[38;5;241m.\u001B[39minfo(\n\u001B[1;32m     90\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msolved after \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtotal_count\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m steps, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtotal_episodes\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m episodes and in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt1\u001B[38;5;241m-\u001B[39mt0\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124ms.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     91\u001B[0m )\n\u001B[1;32m     93\u001B[0m \u001B[38;5;66;03m# Save reconding\u001B[39;00m\n\u001B[0;32m---> 94\u001B[0m record_env\u001B[38;5;241m.\u001B[39mrollout(max_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m, policy\u001B[38;5;241m=\u001B[39mpolicy)\n\u001B[1;32m     95\u001B[0m video_recorder\u001B[38;5;241m.\u001B[39mdump()\n",
      "File \u001B[0;32m~/ntua/Robotalk/rl/torchrl/envs/common.py:2503\u001B[0m, in \u001B[0;36mEnvBase.rollout\u001B[0;34m(self, max_steps, policy, callback, auto_reset, auto_cast_to_device, break_when_any_done, return_contiguous, tensordict, out)\u001B[0m\n\u001B[1;32m   2493\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m   2494\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtensordict\u001B[39m\u001B[38;5;124m\"\u001B[39m: tensordict,\n\u001B[1;32m   2495\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto_cast_to_device\u001B[39m\u001B[38;5;124m\"\u001B[39m: auto_cast_to_device,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2500\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallback\u001B[39m\u001B[38;5;124m\"\u001B[39m: callback,\n\u001B[1;32m   2501\u001B[0m }\n\u001B[1;32m   2502\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m break_when_any_done:\n\u001B[0;32m-> 2503\u001B[0m     tensordicts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_rollout_stop_early(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   2504\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2505\u001B[0m     tensordicts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_rollout_nonstop(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/ntua/Robotalk/rl/torchrl/envs/common.py:2542\u001B[0m, in \u001B[0;36mEnvBase._rollout_stop_early\u001B[0;34m(self, tensordict, auto_cast_to_device, max_steps, policy, policy_device, env_device, callback)\u001B[0m\n\u001B[1;32m   2540\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2541\u001B[0m         tensordict\u001B[38;5;241m.\u001B[39mclear_device_()\n\u001B[0;32m-> 2542\u001B[0m tensordict \u001B[38;5;241m=\u001B[39m policy(tensordict)\n\u001B[1;32m   2543\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m auto_cast_to_device:\n\u001B[1;32m   2544\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m env_device \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/rl_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/rl_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/ntua/Robotalk/tensordict/tensordict/nn/common.py:289\u001B[0m, in \u001B[0;36mdispatch.__call__.<locals>.wrapper\u001B[0;34m(_self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    287\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(out[key] \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m dest)\n\u001B[1;32m    288\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m out\n\u001B[0;32m--> 289\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(_self, tensordict, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/ntua/Robotalk/tensordict/tensordict/_contextlib.py:126\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 126\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/ntua/Robotalk/tensordict/tensordict/nn/utils.py:261\u001B[0m, in \u001B[0;36mset_skip_existing.__call__.<locals>.wrapper\u001B[0;34m(_self, tensordict, *args, **kwargs)\u001B[0m\n\u001B[1;32m    255\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    256\u001B[0m     skip_existing()\n\u001B[1;32m    257\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(key \u001B[38;5;129;01min\u001B[39;00m tensordict\u001B[38;5;241m.\u001B[39mkeys(\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m out_keys)\n\u001B[1;32m    258\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28many\u001B[39m(key \u001B[38;5;129;01min\u001B[39;00m out_keys \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m in_keys)\n\u001B[1;32m    259\u001B[0m ):\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tensordict\n\u001B[0;32m--> 261\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(_self, tensordict, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/ntua/Robotalk/tensordict/tensordict/nn/sequence.py:428\u001B[0m, in \u001B[0;36mTensorDictSequential.forward\u001B[0;34m(self, tensordict, tensordict_out, **kwargs)\u001B[0m\n\u001B[1;32m    426\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs):\n\u001B[1;32m    427\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule:\n\u001B[0;32m--> 428\u001B[0m         tensordict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_module(module, tensordict, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    429\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    430\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    431\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTensorDictSequential does not support keyword arguments other than \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtensordict_out\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or in_keys: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_keys\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkwargs\u001B[38;5;241m.\u001B[39mkeys()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    432\u001B[0m     )\n",
      "File \u001B[0;32m~/ntua/Robotalk/tensordict/tensordict/nn/sequence.py:409\u001B[0m, in \u001B[0;36mTensorDictSequential._run_module\u001B[0;34m(self, module, tensordict, **kwargs)\u001B[0m\n\u001B[1;32m    400\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_module\u001B[39m(\n\u001B[1;32m    401\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    402\u001B[0m     module: TensorDictModule,\n\u001B[1;32m    403\u001B[0m     tensordict: TensorDictBase,\n\u001B[1;32m    404\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    405\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    406\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpartial_tolerant \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\n\u001B[1;32m    407\u001B[0m         key \u001B[38;5;129;01min\u001B[39;00m tensordict\u001B[38;5;241m.\u001B[39mkeys(include_nested\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39min_keys\n\u001B[1;32m    408\u001B[0m     ):\n\u001B[0;32m--> 409\u001B[0m         tensordict \u001B[38;5;241m=\u001B[39m module(tensordict, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    410\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpartial_tolerant \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensordict, LazyStackedTensorDict):\n\u001B[1;32m    411\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m sub_td \u001B[38;5;129;01min\u001B[39;00m tensordict\u001B[38;5;241m.\u001B[39mtensordicts:\n",
      "File \u001B[0;32m~/anaconda3/envs/rl_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/rl_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/ntua/Robotalk/tensordict/tensordict/nn/common.py:289\u001B[0m, in \u001B[0;36mdispatch.__call__.<locals>.wrapper\u001B[0;34m(_self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    287\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(out[key] \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m dest)\n\u001B[1;32m    288\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m out\n\u001B[0;32m--> 289\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(_self, tensordict, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/ntua/Robotalk/tensordict/tensordict/_contextlib.py:126\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 126\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/ntua/Robotalk/tensordict/tensordict/nn/utils.py:261\u001B[0m, in \u001B[0;36mset_skip_existing.__call__.<locals>.wrapper\u001B[0;34m(_self, tensordict, *args, **kwargs)\u001B[0m\n\u001B[1;32m    255\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    256\u001B[0m     skip_existing()\n\u001B[1;32m    257\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(key \u001B[38;5;129;01min\u001B[39;00m tensordict\u001B[38;5;241m.\u001B[39mkeys(\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m out_keys)\n\u001B[1;32m    258\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28many\u001B[39m(key \u001B[38;5;129;01min\u001B[39;00m out_keys \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m in_keys)\n\u001B[1;32m    259\u001B[0m ):\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tensordict\n\u001B[0;32m--> 261\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(_self, tensordict, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/ntua/Robotalk/tensordict/tensordict/nn/common.py:1224\u001B[0m, in \u001B[0;36mTensorDictModule.forward\u001B[0;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1222\u001B[0m in_keys \u001B[38;5;241m=\u001B[39m indent(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124min_keys=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_keys\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m4\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1223\u001B[0m out_keys \u001B[38;5;241m=\u001B[39m indent(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mout_keys=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_keys\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m4\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1224\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m err \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   1225\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTensorDictModule failed with operation\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmodule\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00min_keys\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mout_keys\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1226\u001B[0m )\n",
      "File \u001B[0;32m~/ntua/Robotalk/tensordict/tensordict/nn/common.py:1198\u001B[0m, in \u001B[0;36mTensorDictModule.forward\u001B[0;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1192\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\n\u001B[1;32m   1193\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSome tensors that are necessary for the module call may \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1194\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnot have not been found in the input tensordict: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1195\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe following inputs are None: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnone_set\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1196\u001B[0m         ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   1197\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1198\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m err\n\u001B[1;32m   1199\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensors, (\u001B[38;5;28mdict\u001B[39m, TensorDictBase)):\n\u001B[1;32m   1200\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensors, \u001B[38;5;28mdict\u001B[39m):\n",
      "File \u001B[0;32m~/ntua/Robotalk/tensordict/tensordict/nn/common.py:1184\u001B[0m, in \u001B[0;36mTensorDictModule.forward\u001B[0;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1182\u001B[0m     tensors \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(tensordict\u001B[38;5;241m.\u001B[39mget(in_key, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;28;01mfor\u001B[39;00m in_key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_keys)\n\u001B[1;32m   1183\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1184\u001B[0m     tensors \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_module(tensors, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1185\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m   1186\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(tensor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m tensor \u001B[38;5;129;01min\u001B[39;00m tensors) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNone\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(err):\n",
      "File \u001B[0;32m~/ntua/Robotalk/tensordict/tensordict/nn/common.py:1141\u001B[0m, in \u001B[0;36mTensorDictModule._call_module\u001B[0;34m(self, tensors, **kwargs)\u001B[0m\n\u001B[1;32m   1138\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call_module\u001B[39m(\n\u001B[1;32m   1139\u001B[0m     \u001B[38;5;28mself\u001B[39m, tensors: Sequence[Tensor], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any\n\u001B[1;32m   1140\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor \u001B[38;5;241m|\u001B[39m Sequence[Tensor]:\n\u001B[0;32m-> 1141\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule(\u001B[38;5;241m*\u001B[39mtensors, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/anaconda3/envs/rl_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/rl_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/ntua/Robotalk/rl/torchrl/modules/models/models.py:297\u001B[0m, in \u001B[0;36mMLP.forward\u001B[0;34m(self, *inputs)\u001B[0m\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(inputs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    295\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m (torch\u001B[38;5;241m.\u001B[39mcat([\u001B[38;5;241m*\u001B[39minputs], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m),)\n\u001B[0;32m--> 297\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mforward(\u001B[38;5;241m*\u001B[39minputs)\n\u001B[1;32m    298\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_features, Number):\n\u001B[1;32m    299\u001B[0m     out \u001B[38;5;241m=\u001B[39m out\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m*\u001B[39mout\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_features)\n",
      "File \u001B[0;32m~/anaconda3/envs/rl_env/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m module(\u001B[38;5;28minput\u001B[39m)\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/rl_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/rl_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/rl_env/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mlinear(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (1x4 and 6x64)"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensordict.nn import TensorDictModule as Mod, TensorDictSequential as Seq\n",
    "\n",
    "# Policy\n",
    "\n",
    "from torchrl.modules import EGreedyModule, MLP, QValueModule\n",
    "\n",
    "value_mlp = MLP(out_features=env.action_spec.shape[-1], num_cells=[64, 64])\n",
    "value_net = Mod(value_mlp, in_keys=[\"observation\"], out_keys=[\"action_value\"])\n",
    "policy = Seq(value_net, QValueModule(env.action_spec))\n",
    "exploration_module = EGreedyModule(env.action_spec, annealing_num_steps=100_000, eps_init=0.5)\n",
    "policy_explore = Seq(policy, exploration_module)\n",
    "\n",
    "# Data collector\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import LazyTensorStorage, ReplayBuffer\n",
    "\n",
    "init_rand_steps = 5000\n",
    "frames_per_batch = 100\n",
    "optim_steps = 10\n",
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=-1,\n",
    "    init_random_frames=init_rand_steps\n",
    ")\n",
    "rb = ReplayBuffer(storage=LazyTensorStorage(100_000))\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Optimization & Loss Module\n",
    "\n",
    "from torchrl.objectives import DQNLoss, SoftUpdate\n",
    "\n",
    "loss = DQNLoss(value_network=policy, action_space=env.action_spec, delay_value=True)\n",
    "optim = Adam(loss.parameters(), lr=0.02)\n",
    "updater = SoftUpdate(loss, eps=0.99)\n",
    "\n",
    "# Logger\n",
    "\n",
    "from torchrl._utils import logger as torchrl_logger\n",
    "from torchrl.record import CSVLogger, VideoRecorder\n",
    "\n",
    "path = \"./training_loop\"\n",
    "logger = CSVLogger(exp_name=\"dqn\", log_dir=path, video_format=\"mp4\")\n",
    "video_recorder = VideoRecorder(logger, tag=\"video\")\n",
    "record_env = TransformedEnv(GymEnv(\"CartPole-v1\", from_pixels=True, pixels_only=False), video_recorder)\n",
    "\n",
    "# The main loop\n",
    "total_count = 0\n",
    "total_episodes = 0\n",
    "t0 = time.time()\n",
    "for i, data in enumerate(collector):\n",
    "    # Write data in rb\n",
    "    rb.extend(data)\n",
    "    max_length = rb[:]['next', 'step_count'].max()\n",
    "    if len(rb) > init_rand_steps:\n",
    "        # Optim loop (we do several optim steps per batch collected for efficiency\n",
    "        for _ in range(optim_steps):\n",
    "            sample = rb.sample(128)\n",
    "            loss_vals = loss(sample)\n",
    "            loss_vals[\"loss\"].backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            # Update the exploration factor\n",
    "            exploration_module.step(data.numel())\n",
    "            # Update target params\n",
    "            updater.step()\n",
    "            if i % 10:\n",
    "                torchrl_logger.info(f\"Max num steps: {max_length}, rb length {len(rb)}\")\n",
    "            total_count += data.numel()\n",
    "            total_episodes += data[\"next\", \"done\"].sum()\n",
    "    if max_length > 200:\n",
    "        break # truncate it\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "torchrl_logger.info(\n",
    "    f\"solved after {total_count} steps, {total_episodes} episodes and in {t1-t0}s.\"\n",
    ")\n",
    "\n",
    "# Save reconding\n",
    "record_env.rollout(max_steps=1000, policy=policy)\n",
    "video_recorder.dump()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-27T09:15:34.197548679Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9f3300f2a7d6816c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
