{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33820b6ea9075e77",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-26T16:29:38.741418109Z",
     "start_time": "2024-03-26T16:29:38.720547733Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import gym\n",
    "import torch as th\n",
    "import torch.nn\n",
    "from tensordict import TensorDictBase, TensorDict\n",
    "from torchrl.envs import EnvBase, step_mdp, check_env_specs\n",
    "from torchrl.data import UnboundedContinuousTensorSpec, CompositeSpec, BoundedTensorSpec, BinaryDiscreteTensorSpec\n",
    "\n",
    "\"\"\"\n",
    "# Info for CartPole-v1 test\n",
    "    | Num | Action                 |\n",
    "    |-----|------------------------|\n",
    "    | 0   | Push cart to the left  |\n",
    "    | 1   | Push cart to the right |\n",
    "\n",
    "    | Num | Observation           | Min                 | Max               |\n",
    "    |-----|-----------------------|---------------------|-------------------|\n",
    "    | 0   | Cart Position         | -4.8                | 4.8               |\n",
    "    | 1   | Cart Velocity         | -Inf                | Inf               |\n",
    "    | 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
    "    | 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class CartPoleWrapper(EnvBase):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(device=\"cpu\")\n",
    "        self.env = env  # The underlying environment\n",
    "        # Define the observation and action specs according to the wrapped environment\n",
    "        self.observation_spec = CompositeSpec({\"observation\": BoundedTensorSpec(\n",
    "            low=th.tensor([-4.8, -th.inf, -0.418, -th.inf], dtype=th.float64),\n",
    "            high=th.tensor([4.8, th.inf, 0.418, th.inf], dtype=th.float64))})\n",
    "        self.action_spec = CompositeSpec({\"action\": BinaryDiscreteTensorSpec(1)})\n",
    "        self.reward_spec = CompositeSpec({\"reward\": BoundedTensorSpec(th.tensor([0]), th.inf)})\n",
    "\n",
    "    def _step(self, tensordict: TensorDictBase) -> TensorDictBase:\n",
    "        # Extract the action from the tensordict\n",
    "        action = tensordict.get(\"action\").detach().cpu().numpy()\n",
    "        # Step the underlying environment with the extracted action\n",
    "        obs, reward, term, trunc, info = self.env.step(int(action))\n",
    "        done = term\n",
    "        # print(f\"obs is {obs}\")\n",
    "        # Create a TensorDict to return, including the new observation, reward, and done flag\n",
    "        out = TensorDict({\n",
    "            \"done\": th.tensor([done], dtype=th.bool),\n",
    "            \"observation\": th.tensor(obs, dtype=th.float32),\n",
    "            \"reward\": th.tensor([reward], dtype=th.float32),\n",
    "        }, batch_size=[])\n",
    "        return out\n",
    "\n",
    "    def _reset(self, tensordict: Optional[TensorDictBase] = None, **kwargs) -> TensorDictBase:\n",
    "        # Reset the underlying environment and get the initial observation\n",
    "        obs = self.env.reset()[0]\n",
    "        # Create a TensorDict for the initial state\n",
    "        out = TensorDict({\n",
    "            \"observation\": th.tensor(obs, dtype=th.float32),\n",
    "            \"action\": th.tensor([0], dtype=th.float32),  # Placeholder action\n",
    "            \"done\": th.tensor([False], dtype=th.bool),\n",
    "        }, batch_size=[])\n",
    "        return out\n",
    "\n",
    "    def _set_seed(self, seed: Optional[int]):  # for reproduction of same results\n",
    "        pass\n",
    "        # self.env.seed(seed)  # Assuming the underlying env has a seed method"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing the env"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a45d80c663d15e9"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_57649/3304887409.py:41: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  obs, reward, term, trunc, info = self.env.step(int(action))\n",
      "2024-03-26 18:29:39,726 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('observation', tensor([-0.0322, -0.0097,  0.0372, -0.0105])), ('action', tensor([0.])), ('done', tensor([False])), ('terminated', tensor([False]))])\n"
     ]
    }
   ],
   "source": [
    "gym_env = gym.make('CartPole-v1')\n",
    "env = CartPoleWrapper(gym_env)\n",
    "\n",
    "reset_td = env.reset()\n",
    "print(reset_td.items())\n",
    "\n",
    "step_td = env.step(reset_td)\n",
    "# rollout_td = env.rollout(3)\n",
    "# print(rollout_td)\n",
    "# print('finished rollout')\n",
    "# reset_with_action = env.rand_action(reset_td)\n",
    "# print(reset_with_action[\"action\"])\n",
    "# \n",
    "# data = step_mdp(step_td)\n",
    "# print(data)\n",
    "\n",
    "check_env_specs(env)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T16:29:39.761335866Z",
     "start_time": "2024-03-26T16:29:39.716511143Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the transformation and training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8616c3ffc288748b"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int8, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50345/27740417.py:40: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  obs, reward, term, trunc, info = self.env.step(int(action))\n"
     ]
    }
   ],
   "source": [
    "from torchrl.envs import StepCounter, TransformedEnv\n",
    "\n",
    "transformed_env = TransformedEnv(env, StepCounter(max_steps=10))  # after 10 steps in truncated the env\n",
    "rollout = transformed_env.rollout(max_steps=100)\n",
    "print(rollout)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T13:16:58.110912798Z",
     "start_time": "2024-03-26T13:16:58.020698856Z"
    }
   },
   "id": "b28b1baf5a124ee4"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True]])\n"
     ]
    }
   ],
   "source": [
    "print(rollout[\"next\", \"truncated\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T13:16:58.111064985Z",
     "start_time": "2024-03-26T13:16:58.020987936Z"
    }
   },
   "id": "b635937aea2b7ad7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now working with Modules"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7095868a221dbdeb"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        actions: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50345/27740417.py:40: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  obs, reward, term, trunc, info = self.env.step(int(action))\n"
     ]
    }
   ],
   "source": [
    "from tensordict.nn import TensorDictModule\n",
    "module = th.nn.LazyLinear(env.action_spec.shape[-1])\n",
    "policy = TensorDictModule(\n",
    "    module, \n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"actions\"],\n",
    ")\n",
    "\n",
    "# todo why do I get a number for 8 to 10 when i run this? does it mean that it loses at 8 steps?\n",
    "rollout = env.rollout(max_steps=10, policy=policy) \n",
    "rollout\n",
    "\n",
    "from torchrl.modules import Actor\n",
    "\n",
    "\"\"\"\n",
    "    Args:\n",
    "        module (nn.Module): a :class:`~torch.nn.Module` used to map the input to\n",
    "            the output parameter space.\n",
    "        in_keys (iterable of str, optional): keys to be read from input\n",
    "            tensordict and passed to the module. If it\n",
    "            contains more than one element, the values will be passed in the\n",
    "            order given by the in_keys iterable.\n",
    "            Defaults to ``[\"observation\"]``.\n",
    "        out_keys (iterable of str): keys to be written to the input tensordict.\n",
    "            The length of out_keys must match the\n",
    "            number of tensors returned by the embedded module. Using ``\"_\"`` as a\n",
    "            key avoid writing tensor to output.\n",
    "            Defaults to ``[\"action\"]``.\n",
    "\"\"\"\n",
    "\n",
    "policy = Actor(module)\n",
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "print(rollout)\n",
    "from torchrl.modules import MLP\n",
    "\n",
    "module = MLP(\n",
    "    out_features=env.action_spec.shape[-1],\n",
    "    num_cells=[32, 64],\n",
    "    activation_class=th.nn.Tanh,\n",
    ")\n",
    "policy = Actor(module)\n",
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "rollout\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch.distributions import Normal\n",
    "from torchrl.modules import ProbabilisticActor\n",
    "\n",
    "backbone = MLP(in_features=4, out_features=2)\n",
    "extractor = NormalParamExtractor()\n",
    "module = th.nn.Sequential(backbone, extractor)\n",
    "td_module = TensorDictModule(module, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"])\n",
    "policy = ProbabilisticActor(\n",
    "    td_module,\n",
    "    in_keys=[\"loc\", \"scale\"],\n",
    "    out_keys=[\"action\"],\n",
    "    distribution_class=Normal,\n",
    "    return_log_prob=True\n",
    ")\n",
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "rollout"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T13:19:38.115542404Z",
     "start_time": "2024-03-26T13:19:38.100651252Z"
    }
   },
   "id": "acd673dc264a6689"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now doing Module testing\n",
    "\n",
    "\n",
    "module = th.nn.LazyLinear(env.action_spec.shape[-1])\n",
    "policy = TensorDictModule(\n",
    "    module,\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"actions\"],\n",
    ")\n",
    "\n",
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "print(rollout)\n",
    "\n",
    "policy = Actor(module)\n",
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "print(rollout)\n",
    "\n",
    "module = MLP(\n",
    "    out_features=env.action_spec.shape[-1],\n",
    "    num_cells=[32, 64],\n",
    "    activation_class=th.nn.Tanh,\n",
    ")\n",
    "policy = Actor(module)\n",
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "print(rollout)\n",
    "\n",
    "backbone = MLP(in_features=4, out_features=2)\n",
    "extractor = NormalParamExtractor()\n",
    "module = th.nn.Sequential(backbone, extractor)\n",
    "td_module = TensorDictModule(module, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"])\n",
    "policy = ProbabilisticActor(\n",
    "    td_module,\n",
    "    in_keys=[\"loc\", \"scale\"],\n",
    "    out_keys=[\"action\"],\n",
    "    distribution_class=Normal,\n",
    "    return_log_prob=True\n",
    ")\n",
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "print(rollout)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9b35e7e984f4321"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
